First, log on the dumbo.

Transfer data from mySQL to dumbo. The flag show my directory on my HDFS: --target-dir ./project_BDAD/candidatesBDAD, you can modify it

candidatesBDAD:
sqoop import --connect jdbc:mysql://avaultdbprod.cjtcotq8rbjh.us-east-1.rds.amazonaws.com/itwallstreet?zeroDateTimeBehavior=CONVERT_TO_NULL --username andiamop --password *Cupertino1 --target-dir ./project_BDAD/candidatesBDAD  --table candidatesBDAD -m 1\

---------------------------------------------------------------------
# test for delimiters
sqoop import --connect jdbc:mysql://avaultdbprod.cjtcotq8rbjh.us-east-1.rds.amazonaws.com/itwallstreet?zeroDateTimeBehavior=CONVERT_TO_NULL --username andiamop --password *Cupertino1 --target-dir ./project_BDAD/candidatesBDAD  --table candidatesBDAD --split-by candidateId --mysql-delimiters



-----------------------------------------------------------------------

jobOrdersBDAD:
sqoop import --connect jdbc:mysql://avaultdbprod.cjtcotq8rbjh.us-east-1.rds.amazonaws.com/itwallstreet?zeroDateTimeBehavior=CONVERT_TO_NULL --username andiamop --password *Cupertino1 --target-dir ./project_BDAD/jobOrdersBDAD  --table jobOrdersBDAD -m 1\


placementsBDAD:
sqoop import --connect jdbc:mysql://avaultdbprod.cjtcotq8rbjh.us-east-1.rds.amazonaws.com/itwallstreet?zeroDateTimeBehavior=CONVERT_TO_NULL --username andiamop --password *Cupertino1 --target-dir ./project_BDAD/placementsBDAD  --table  placementsBDAD -m 1\


submittalTrackingBDAD:
sqoop import --connect jdbc:mysql://avaultdbprod.cjtcotq8rbjh.us-east-1.rds.amazonaws.com/itwallstreet?zeroDateTimeBehavior=CONVERT_TO_NULL --username andiamop --password *Cupertino1 --target-dir ./project_BDAD/submittalTrackingBDAD  --table submittalTrackingBDAD -m 1\



Second, import these data into spark.
--------------------------------------------------------------------
candidatesBDAD:
val myUDf = udf((s:String) => s.split(","))

# val df = spark.sqlContext.read.json("./project_BDAD/candidatePart")

val df = spark.sqlContext.read.json("./project_BDAD/candidatesBDAD")
val newDF = df.withColumn("splitted", myUDf(df("_corrupt_record")))
val candidatesDF = newDF.withColumn("candidateId", newDF("splitted").getItem(0)).withColumn("email", newDF("splitted").getItem(1)).withColumn("gender", newDF("splitted").getItem(2)).withColumn("isMinority", newDF("splitted").getItem(3)).withColumn("isIvy", newDF("splitted").getItem(4)).withColumn("recruiterId", newDF("splitted").getItem(5)).withColumn("candidateCreationDate", newDF("splitted").getItem(6)).withColumn("zip", newDF("splitted").getItem(7)).withColumn("isWillingRelocate", newDF("splitted").getItem(8)).withColumn("clarity", newDF("splitted").getItem(9)).withColumn("personability", newDF("splitted").getItem(10)).withColumn("yearBeganExperience", newDF("splitted").getItem(11)).withColumn("i9", newDF("splitted").getItem(12)).withColumn("isPermanent", newDF("splitted").getItem(13)).withColumn("isConsulting", newDF("splitted").getItem(14)).withColumn("resume_text", newDF("splitted").getItem(15)).withColumn("mainCategory", newDF("splitted").getItem(16)).withColumn("funcCategory", newDF("splitted").getItem(17)).withColumn("techCategory", newDF("splitted").getItem(18)).withColumn("writeup", newDF("splitted").getItem(19)).drop("_corrupt_record").drop("splitted")


candidatesDF.select("candidateId","email","gender","isMinority","isIvy","candidateCreationDate","zip","isWillingRelocate").show(10)
+-----------+--------------------+------+----------+-----+---------------------+-----+-----------------+
|candidateId|               email|gender|isMinority|isIvy|candidateCreationDate|  zip|isWillingRelocate|
+-----------+--------------------+------+----------+-----+---------------------+-----+-----------------+
|      76152|adam.moshir@gmail...|     M|      true| true| 2018-07-16 15:23:...|94016|             true|
|      76151|       l;askj@ya.com|     F|      true| true| 2018-07-16 14:20:...|10004|             true|
|      76150|venkateshais@yaho...|     M|      true| true| 2018-07-16 13:37:...|32901|            false|
|      76149|rm_pasha@hotmail.com|     M|      true| true| 2018-07-16 13:31:...|10001|            false|
|      76148|frodriguez3377@gm...|     M|      true| true| 2018-07-16 13:28:...|94016|             true|
|      76147|saiyang.qi@gmail.com|     M|      true| true| 2018-07-16 13:18:...|10001|            false|
|      76146|rakeshvyas.vyas@g...|     M|      true| true| 2018-07-16 13:18:...|33601|             true|
|      76145|christian.loris@g...|     M|      true| true| 2018-07-16 13:17:...|32901|            false|
|      76144|sree.ramg88@gmail...|     M|      true| true| 2018-07-16 13:07:...|60169|            false|
|      76143|naveenhadoop87@gm...|     M|      true| true| 2018-07-16 13:01:...|41703|             true|
+-----------+--------------------+------+----------+-----+---------------------+-----+-----------------+

candidatesDF.select("clarity","personability","yearBeganExperience","i9","isPermanent","isConsulting","resume_text","mainCategory").show(10)
+-------+-------------+-------------------+----------------+-----------+------------+--------------------+--------------------+
|clarity|personability|yearBeganExperience|              i9|isPermanent|isConsulting|         resume_text|        mainCategory|
+-------+-------------+-------------------+----------------+-----------+------------+--------------------+--------------------+
|      5|            5|               2012|Eligible(USC-GC)|       true|       false|                    |Software Engineering|
|      0|            0|               2017|         Unknown|       true|       false|                    |Software Engineering|
|      5|            5|               2013|           Other|       true|       false|                null|Software Engineering|
|      5|            4|               2001|Eligible(USC-GC)|      false|       false|                null|Software Engineering|
|      4|            5|               1997|Eligible(USC-GC)|       true|       false|<p><p class="MsoN...|               Other|
|      5|            4|               2017|Eligible(USC-GC)|       true|       false|                null|Software Engineering|
|      5|            5|               2006|        H1B Visa|       true|       false|                null|Software Engineering|
|      5|            5|               1999|Eligible(USC-GC)|       true|       false|                null|Software Engineering|
|      4|            4|               2010|        H1B Visa|       true|       false|                null|Software Engineering|
|      5|            5|               2014|        TN Visaa|       true|       false|                null|Software Engineering|
+-------+-------------+-------------------+----------------+-----------+------------+--------------------+--------------------+

scala> candidatesDF.select("funcCategory","techCategory","writeup").show(10)
+---------------+--------------------+--------------------+
|   funcCategory|        techCategory|             writeup|
+---------------+--------------------+--------------------+
| Data Scientist|           Python-OO|<p><strong>Develo...|
|Back-End/Server|                Java|<p><strong>Develo...|
|Back-End/Server|              C#/Web|                 C++|
|Back-End/Server|                 C++|<p><strong>Develo...|
|  Trade Support|    Active Directory|<p><strong>Develo...|
|Back-End/Server|           Python-OO|<p><strong>Develo...|
|Back-End/Server|                J2EE|                Java|
|Back-End/Server| Database Develop...|              C#/Web|
|      Front-End|          JavaScript|<p><p class="MsoN...|
|  Data Engineer|           Python-OO|<p><strong>Develo...|
+---------------+--------------------+--------------------+
--------------------------------------------------------------------------

jobOrdersBDAD:
val myUDf = udf((s:String) => s.split(","))
val df_jobOrdersBDAD = spark.sqlContext.read.json("./project_BDAD/jobOrdersBDAD")
val newDF = df_jobOrdersBDAD.withColumn("splitted",myUDf(df_jobOrdersBDAD("_corrupt_record")))

val jobOrdersDF = newDF.withColumn("job_order_id", newDF("splitted").getItem(0)).withColumn("title", newDF("splitted").getItem(1)).withColumn("title_publish", newDF("splitted").getItem(2)).withColumn("accmanager_id", newDF("splitted").getItem(3)).withColumn("client_id", newDF("splitted").getItem(4)).withColumn("is_permanent", newDF("splitted").getItem(5)).withColumn("is_consulting", newDF("splitted").getItem(6)).withColumn("job_description", newDF("splitted").getItem(7)).withColumn("job_description_internal", newDF("splitted").getItem(8)).withColumn("internal_notes", newDF("splitted").getItem(9)).withColumn("min_years_experience", newDF("splitted").getItem(10)).withColumn("openings", newDF("splitted").getItem(11)).withColumn("is_event", newDF("splitted").getItem(12)).withColumn("job_order_status", newDF("splitted").getItem(13)).withColumn("min_salary", newDF("splitted").getItem(14)).withColumn("max_salary", newDF("splitted").getItem(15)).withColumn("salarytype_id", newDF("splitted").getItem(16)).withColumn("salary_notes", newDF("splitted").getItem(17)).withColumn("mainCategory", newDF("splitted").getItem(18)).withColumn("functionalCategory",
newDF("splitted").getItem(18)).withColumn("technicalCategory", 
newDF("splitted").getItem(19)).drop("_corrupt_record").drop("splitted")


jobOrdersDF.select("job_order_id","title","title_publish","accmanager_id","client_id","is_permanent","is_consulting","job_description","job_description_internal").show(10)
+------------+--------------------+--------------------+-------------+------------------+------------+-------------+---------------+------------------------+
|job_order_id|               title|       title_publish|accmanager_id|         client_id|is_permanent|is_consulting|job_description|job_description_internal|
+------------+--------------------+--------------------+-------------+------------------+------------+-------------+---------------+------------------------+
|       10000|Cloud/Systems Mon...|Cloud Monitoring ...|           70|               571|        true|        false|              0|                    true|
|        9999|Data Scientist fo...|Data Scientist fo...|           70|               637|        true|        false|              0|                    true|
|        9998|Engineering Manag...|  Engineering Man...|          104|               634|        true|        false|              7|                    true|
|        9997|Senior Frontend E...|Senior Frontend E...|          104|               634|        true|        false|              5|                    true|
|        9996|          Consultant|   Software Engineer|   Consultant| Software Engineer|         122|          589|          false|                    true|
|        9995|Software Engineer...|Software Engineer...|          128|               639|        true|        false|              2|                    true|
|        9994|Full Stack .NET d...|Full Stack .NET d...|          166|               498|        true|        false|              3|                    true|
|        9993|Senior C++ Develo...|Senior C++ Develo...|           70|               637|        true|        false|              0|                    true|
|        9992|Lead Full-Stack D...|Lead Full-Stack D...|           70|               637|        true|        false|              0|                    true|
|        9991|GE Digital Employ Q3|GE Digital Employ Q3|            3|               621|        true|        false|              0|                    true|
+------------+--------------------+--------------------+-------------+------------------+------------+-------------+---------------+------------------------+
jobOrdersDF.select("internal_notes","min_years_experience","openings","is_event","job_order_status","min_salary","max_salary","salarytype_id","salary_notes").show(10)
+--------------+--------------------+--------+--------+----------------+----------+--------------------+--------------------+--------------------+
|internal_notes|min_years_experience|openings|is_event|job_order_status|min_salary|          max_salary|       salarytype_id|        salary_notes|
+--------------+--------------------+--------+--------+----------------+----------+--------------------+--------------------+--------------------+
|         false|                true|     120|     125|               0|      null|  Operations/Support|Systems Administr...|                null|
|         false|                true|     120|     130|               0|      null|Software Engineering|      Data Scientist|           Python-OO|
|         false|                true|     200|     300|               0|      null|Software Engineering|     Back-End/Server|           Front-End|
|         false|                true|       0|       0|               0|      null|Software Engineering|           Front-End|          JavaScript|
|             3|                true|   false|    true|              50|       100|                   0|                null|Software Engineering|
|         false|                true|     120|     180|               0|      null|Software Engineering|     Back-End/Server|                J2EE|
|         false|                true|      90|     170|               0|      null|Software Engineering|                null|              C#/Web|
|         false|                true|     140|     165|               0|      null|Software Engineering|     Back-End/Server|                 C++|
|         false|                true|       0|       0|               0|      null|Software Engineering|     Back-End/Server|           Front-End|
|         false|                true|       1|       2|               0|      null|Software Engineering|     DevOps Engineer|                Java|
+--------------+--------------------+--------+--------+----------------+----------+--------------------+--------------------+--------------------+
+---------------+------------------+-----------------+
|   mainCategory|functionalCategory|technicalCategory|
+---------------+------------------+-----------------+
|           null|              null|             null|
|           null|              null|             null|
| Tech/Team Lead|    Tech/Team Lead|              C++|
|       React.js|          React.js|             null|
|Back-End/Server|   Back-End/Server|           C#/Web|
|           Java|              Java|             null|
|     JavaScript|        JavaScript|             null|
|           null|              null|             null|
| Tech/Team Lead|    Tech/Team Lead|             Java|
|           null|              null|             null|
+---------------+------------------+-----------------+																			

--------------------------------------------------------------------------
placementsBDAD:

val myUDf = udf((s:String) => s.split(","))
val df_placementsBDAD = spark.sqlContext.read.json("./project_BDAD/placementsBDAD")
val newDF = df_placementsBDAD.withColumn("splitted",myUDf(df_placementsBDAD("_corrupt_record")))
val placementsDF = newDF.withColumn("placement_id", newDF("splitted").getItem(0)).withColumn("placement_type", newDF("splitted").getItem(1)).withColumn("placement_date", newDF("splitted").getItem(2)).withColumn("candidate_id", newDF("splitted").getItem(3)).withColumn("joborder_id", newDF("splitted").getItem(4)).withColumn("salary", newDF("splitted").getItem(5)).withColumn("fee", newDF("splitted").getItem(6)).withColumn("notes", newDF("splitted").getItem(8)).withColumn("accmanager_id",  newDF("splitted").getItem(9)).drop("_corrupt_record").drop("splitted")


placementsDF.select("placement_id","placement_type","placement_date","candidate_id","joborder_id","salary","fee","notes","accmanager_id").show(10)
+------------+--------------+--------------+------------+-----------+-------+-------+-----+-------------+
|placement_id|placement_type|placement_date|candidate_id|joborder_id| salary|    fee|notes|accmanager_id|
+------------+--------------+--------------+------------+-----------+-------+-------+-----+-------------+
|        1371|     Permanent|    2018-07-08|       76151|       9991|   null|19445.0| null|         null|
|        1370|     Permanent|    2018-07-09|       75971|       9991|   null| 8333.0| null|         null|
|        1369|     Permanent|    2018-06-18|       75449|       9750|    1.0| 8333.0| null|         null|
|        1368|     Permanent|    2018-07-09|       75284|       9991|16666.0| 8333.0| null|         null|
|        1367|     Permanent|    2018-06-11|       75284|       9912|    1.0| 8333.0| null|         null|
|        1366|     Permanent|    2018-07-09|       75283|       9991|   null| 8333.0| null|         null|
|        1365|     Permanent|    2018-06-11|       75283|       9912|    1.0| 8333.0| null|         null|
|        1364|     Permanent|    2018-07-09|       75282|       9991|16666.0| 8333.0| null|         null|
|        1363|     Permanent|    2018-06-11|       75282|       9912|    1.0| 8333.0| null|         null|
|        1362|     Permanent|    2018-07-09|       75281|       9991|   null| 8333.0| null|         null|
+------------+--------------+--------------+------------+-----------+-------+-------+-----+-------------+






--------------------------------------------------------------------------
submittalTrackingBDAD:
val myUDf = udf((s:String) => s.split(","))
val df_submittalTrackingBDAD = spark.sqlContext.read.json("./project_BDAD/submittalTrackingBDAD")
val newDF = df_submittalTrackingBDAD.withColumn("splitted",myUDf(df_submittalTrackingBDAD("_corrupt_record")))

val submittalTrackingDF = newDF.withColumn("change_id", newDF("splitted").getItem(0)).withColumn("candidate_id", newDF("splitted").getItem(1)).withColumn("joborder_id", newDF("splitted").getItem(2)).withColumn("status_id", newDF("splitted").getItem(3)).withColumn("date_updated", newDF("splitted").getItem(4)).withColumn("date_interview", newDF("splitted").getItem(5)).withColumn("interview_type", newDF("splitted").getItem(6)).withColumn("notes", newDF("splitted").getItem(7)).withColumn("feedback", 
newDF("splitted").getItem(8)).withColumn("clfeedback",
newDF("splitted").getItem(9)).drop("_corrupt_record").drop("splitted")

placementsDF.select("change_id","candidate_id","joborder_id","status_id","date_updated","date_interview","interview_type","notes","feedback","clfeedback").show(10)
+---------+------------+-----------+---------+--------------------+--------------+--------------+-----+--------+----------+
|change_id|candidate_id|joborder_id|status_id|        date_updated|date_interview|interview_type|notes|feedback|clfeedback|
+---------+------------+-----------+---------+--------------------+--------------+--------------+-----+--------+----------+
|   356992|       42508|       5240|        1|2012-08-28 12:26:...|          null|          null| null|    null|      null|
|   356991|       43274|       5297|        1|2012-11-29 07:54:...|          null|          null| null|    null|      null|
|   356990|       46161|       5837|        1|2013-10-11 13:51:...|          null|          null| null|    null|      null|
|   356989|       48533|       6164|        1|2014-05-05 10:45:...|          null|          null| null|    null|      null|
|   356988|       51148|       6935|        1|2015-08-07 09:52:...|          null|          null| null|    null|      null|
|   356987|       64550|       8711|        1|2017-03-17 12:53:...|          null|          null| null|    null|      null|
|   356986|       64557|       8784|        1|2017-04-27 10:06:...|          null|          null| null|    null|      null|
|   356985|       66337|       8960|        1|2017-06-23 09:13:...|          null|          null| null|    null|      null|
|   356984|       66643|       9000|        1|2017-07-10 05:38:...|          null|          null| null|    null|      null|
|   356983|       62162|       9014|        1|2017-07-19 07:33:...|          null|          null| null|    null|      null|
+---------+------------+-----------+---------+--------------------+--------------+--------------+-----+--------+----------+
--------------------------------------------------------------------------